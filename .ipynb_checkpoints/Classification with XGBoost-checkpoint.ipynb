{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import pandas as pd \n",
    "import nltk as nltk\n",
    "import  xgboost, numpy, string\n",
    "import datetime as dt\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer to check execution timing for each function call # \n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = dt.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((dt.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "# Spelling checker # \n",
    "def spell_correct(array):\n",
    "    spell = SpellChecker()\n",
    "    for i in range(len(array)):\n",
    "        array[i] = spell.correction(array[i])\n",
    "    return array\n",
    "    \n",
    "def stem(array):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in array]\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    \n",
    "    # Fit the training dataset onto classifier #\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # Predict the labels on validation dataset #\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer: \n",
      " Time taken: 0 hours 1 minutes and 38.58 seconds.\n",
      "\n",
      "Stemming: \n",
      " Time taken: 0 hours 4 minutes and 45.6 seconds.\n",
      "\n",
      "Train-Test Split: \n",
      " Time taken: 0 hours 0 minutes and 0.24 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Data Prep# \n",
    "df = pd.read_csv(\"data/cleaned_hotelreviews.csv\")\n",
    "\n",
    "# Drop rows with null comments # \n",
    "df = df.dropna(how='any', axis=0)\n",
    "\n",
    "# Make words case-insensitive # \n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# Remove punctuations if any # \n",
    "df[\"words_only\"] = df['reviews'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Tokenization with NLTK # \n",
    "start_time = timer(None)\n",
    "df['tokenized'] = df['words_only'].apply(nltk.word_tokenize)\n",
    "print(\"\\nTokenizer: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Spelling checker # : Replace incorrect words with correct words \n",
    "#start_time = timer(None)\n",
    "#df['corrected'] = df['tokenized'].apply(spell_correct)\n",
    "#print(\"\\nSpelling Correction: \",end=\"\")\n",
    "#timer(start_time)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "start_time = timer(None)\n",
    "df['stemmed'] = df['tokenized'].apply(stem)\n",
    "print(\"\\nStemming: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Turn arrays for each row in df['stemmed'] into a string #: Needed to run SkLearn Lib\n",
    "df['stemmed'] = df['stemmed'].apply(\" \".join)\n",
    "\n",
    "# Train - Test Split # \n",
    "start_time = timer(None)\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['stemmed'], df['class'])\n",
    "print(\"\\nTrain-Test Split: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Label encode target variable to run ML models # \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count vectorisation : Create vectors as features \n",
    "    # Every row represents a review \n",
    "    # Every column represents a term from the corpus \n",
    "    # Every cell represents the frequency count of the particular term in the particular review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectors:\n",
      " Time taken: 0 hours 0 minutes and 23.82 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Create count vectoriser object # \n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['stemmed'])\n",
    "\n",
    "# Transform training and validation data # \n",
    "xtrain_count = count_vector.transform(train_x)\n",
    "xvalid_count = count_vector.transform(valid_x)\n",
    "\n",
    "print(\"Count Vectors:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TF-IDF Vectors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Level TF-IDF\n",
      " Time taken: 0 hours 0 minutes and 24.14 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Word Level TF-IDF #: Matrix represents tf-idf scores of every term in each review \n",
    "tfidf_word = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_word.fit(df['stemmed'])\n",
    "xtrain_tfidf_word = tfidf_word.transform(train_x)\n",
    "xvalid_tfidf_word = tfidf_word.transform(valid_x)\n",
    "\n",
    "print(\"Word Level TF-IDF\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram, Bigram, Trigram TF-IDF:\n",
      " Time taken: 0 hours 1 minutes and 59.54 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Unigram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram = tfidf_unigram.transform(train_x)\n",
    "xvalid_tfidf_unigram = tfidf_unigram.transform(valid_x)\n",
    "\n",
    "# Bigram Level TF-IDF #: Terms are grouped together by twos \n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_bigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_bigram = tfidf_bigram.transform(train_x)\n",
    "xvalid_tfidf_bigram = tfidf_bigram.transform(valid_x)\n",
    "\n",
    "# Trigram Level TF-IDF #: Terms are grouped together in threes \n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_trigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_trigram = tfidf_trigram.transform(train_x)\n",
    "xvalid_tfidf_trigram = tfidf_trigram.transform(valid_x)\n",
    "\n",
    "print(\"Unigram, Bigram, Trigram TF-IDF:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Level TF-IDF:\n",
      " Time taken: 0 hours 3 minutes and 7.4 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Character Level TF-IDF #: Matrix represents tf-idf scores of character level uni, bi & tri-gram of all reviews\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=5000)\n",
    "tfidf_char.fit(df['stemmed'])\n",
    "xtrain_tfidf_char =  tfidf_char.transform(train_x) \n",
    "xvalid_tfidf_char =  tfidf_char.transform(valid_x) \n",
    "\n",
    "print(\"Character Level TF-IDF:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a bit expens date it but worth imperson is bad',\n",
       " 'facil reason attract accompani via more approach immacul extend defin',\n",
       " 'beauti hotel and a with staff veri wonder friendli servic',\n",
       " 'a in extrem park locat great far area help kept',\n",
       " 'control temperatur condit arrog effort unwelcom idea climb wors shout',\n",
       " 'understand bang stabl picadelli stylish citi worh heigh pr explor',\n",
       " 'bit expens a with no valu should have includ price',\n",
       " 'oak panel loung relax beauti a in bathroom linen bath',\n",
       " 'dark sm each greenwich thame gbp kettl head luggag advantag',\n",
       " 'plug couldnt pm housekeep joke suppli home a to move',\n",
       " 'properli due doesn tricki sheet thick refil unless privaci characterist',\n",
       " 'regular updat smartest key basi tuck thoroughli obvious hustl bustl',\n",
       " 'vou ca deplacez va un voitur peu vill si en',\n",
       " 'onsit a nice in would and better have been bath',\n",
       " 'complaint newspap languag post give valu bad a english experi',\n",
       " 'were queue promptli clinic everyon sever comput western fruit replac',\n",
       " 'swim sunbath rooftop bigger area a breakfast to valu better',\n",
       " 'a the bit to and from in far room wa',\n",
       " 'a bit the bath citi from far but hotel subway',\n",
       " 'rambla la bit a far from the for shop room']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LDA Model # \n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vector.get_feature_names()\n",
    "\n",
    "# View Topic Models # \n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "\n",
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    \n",
    "    # Fit the training dataset onto classifier #\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # Predict the labels on validation dataset #\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes indepedence among predictors # \n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"NB, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"NB, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"NB, Tri-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier (Logistic Regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"LR, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"LR, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"LR, Tri-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"SVM, Uni-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Word Level Vectors:  0.816\n",
      "SVM, Uni-Gram Vectors:  0.816\n",
      "SVM, Bi-Gram Vectors:  0.816\n",
      "SVM, Tri-Gram Vectors:  0.816\n",
      "SVM, CharLevel Vectors:  0.816\n"
     ]
    }
   ],
   "source": [
    "# Supervised ML Algo that extracts best possible hyper-plane/ line that segregates the two classes #\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"SVM, Word Level Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"SVM, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"SVM, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"SVM, Tri-Gram Vectors: \", accuracy)\n",
    "       \n",
    "# SVM on Char Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"SVM, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"RF, Word Level Vectors: \", accuracy)\n",
    "\n",
    "# RF on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"RF, Uni-gram TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"RF, Bi-gram TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"RF, Tri-gram TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Char Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"RF, CharLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.9287622226378359\n"
     ]
    }
   ],
   "source": [
    "# Ran this with full data due to resource constraint :)))))))) #\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_word.tocsc(), train_y, xvalid_tfidf_word.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_unigram.tocsc(), train_y, xvalid_tfidf_unigram.tocsc())\n",
    "print (\"Xgb, Unigram TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_bigram.tocsc(), train_y, xvalid_tfidf_bigram.tocsc())\n",
    "print (\"Xgb, Bigram TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_trigram.tocsc(), train_y, xvalid_tfidf_trigram.tocsc())\n",
    "print (\"Xgb, Trigram TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_char.tocsc(), train_y, xvalid_tfidf_char.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Importance XGB \n",
    "###### Can't plot important features if we only have one train column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code for future reference # \n",
    "# Plot graph showing importance features, max = 50 features # \n",
    "importance = xgboost.XGBClassifier().feature_importances_ \n",
    "importance = pd.Series(importance, index=xtrain_count.columns)\n",
    "importance.nlargest(50).plot(kind='barh')\n",
    "\n",
    "# Select important features #\n",
    "importance.sort_values(axis=0,ascending=False, inplace=True)\n",
    "selected_features = importance.index[0:30].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Model # \n",
    "model = xgboost.XGBClassifier(max_depth=7,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.2,\n",
    "                           n_estimators=500,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0,\n",
    "                           max_delta_step=0,\n",
    "                           subsample=1,\n",
    "                           colsample_bytree=1,\n",
    "                           colsample_bylevel=1,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=0,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=1,\n",
    "                           missing=None,\n",
    "                           tree_method='exact',\n",
    "                           nthread=4)\n",
    "\n",
    "# Params for hyperparameter grid search # \n",
    "params = {\n",
    "        'max_depth': [5,7],\n",
    "        'min_child_weight': [1, 5],\n",
    "        'gamma': [0.5, 1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Grid Search with 3 Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# 3 fold validation with hyperparameter grid search #\n",
    "folds = 3\n",
    "param_comb = 3\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle = False, random_state = None)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', n_jobs=3, cv=kf.split(xtrain_count,train_y), \n",
    "                    verbose=3 )\n",
    "\n",
    "start_time = timer(None)\n",
    "\n",
    "grid.fit(xtrain_count,train_y)\n",
    "\n",
    "print('\\n Best estimator:')\n",
    "print(grid.best_estimator_)\n",
    "print('\\n Best score:')\n",
    "print(grid.best_score_)\n",
    "print('\\n Best parameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
