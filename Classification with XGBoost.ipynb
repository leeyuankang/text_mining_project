{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import pandas as pd \n",
    "import nltk as nltk\n",
    "import  xgboost, numpy, string\n",
    "import datetime as dt\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer to check execution timing for each function call # \n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = dt.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((dt.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "# Spelling checker # \n",
    "def spell_correct(array):\n",
    "    spell = SpellChecker()\n",
    "    for i in range(len(array)):\n",
    "        array[i] = spell.correction(array[i])\n",
    "    return array\n",
    "    \n",
    "def stem(array):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in array]\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # Fit the training dataset onto classifier #\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # Predict the labels on validation dataset #\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer: \n",
      " Time taken: 0 hours 3 minutes and 27.75 seconds.\n",
      "\n",
      "Stemming: \n",
      " Time taken: 0 hours 11 minutes and 7.13 seconds.\n",
      "\n",
      "Train-Test Split: \n",
      " Time taken: 0 hours 0 minutes and 0.49 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Data Prep# \n",
    "df = pd.read_csv(\"data/cleaned_hotelreviews.csv\")\n",
    "\n",
    "# Drop rows with null comments # \n",
    "df = df.dropna(how='any', axis=0)\n",
    "\n",
    "# Make words case-insensitive # \n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# Remove punctuations if any # \n",
    "df[\"words_only\"] = df['reviews'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Tokenization with NLTK # \n",
    "start_time = timer(None)\n",
    "df['tokenized'] = df['words_only'].apply(nltk.word_tokenize)\n",
    "print(\"\\nTokenizer: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Spelling checker # : Replace incorrect words with correct words \n",
    "#start_time = timer(None)\n",
    "#df['corrected'] = df['tokenized'].apply(spell_correct)\n",
    "#print(\"\\nSpelling Correction: \",end=\"\")\n",
    "#timer(start_time)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "start_time = timer(None)\n",
    "df['stemmed'] = df['tokenized'].apply(stem)\n",
    "print(\"\\nStemming: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Turn arrays for each row in df['stemmed'] into a string #: Needed to run SkLearn Lib\n",
    "df['stemmed'] = df['stemmed'].apply(\" \".join)\n",
    "\n",
    "# Train - Test Split # \n",
    "start_time = timer(None)\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['stemmed'], df['class'])\n",
    "print(\"\\nTrain-Test Split: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Label encode target variable to run ML models # \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count vectorisation : Create vectors as features \n",
    "    # Every row represents a review \n",
    "    # Every column represents a term from the corpus \n",
    "    # Every cell represents the frequency count of the particular term in the particular review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectors:\n",
      " Time taken: 0 hours 1 minutes and 7.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Create count vectoriser object # \n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['stemmed'])\n",
    "\n",
    "# Transform training and validation data # \n",
    "xtrain_count = count_vector.transform(train_x)\n",
    "xvalid_count = count_vector.transform(valid_x)\n",
    "\n",
    "print(\"Count Vectors:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2417)\t1\n",
      "  (0, 11832)\t1\n",
      "  (0, 24284)\t1\n",
      "  (0, 28487)\t1\n",
      "  (0, 34514)\t1\n",
      "  (0, 48410)\t1\n",
      "  (0, 49270)\t1\n",
      "  (0, 49582)\t1\n",
      "  (1, 1506)\t1\n",
      "  (1, 17408)\t1\n",
      "  (2, 139)\t1\n",
      "  (2, 2033)\t2\n",
      "  (2, 7887)\t1\n",
      "  (2, 12141)\t1\n",
      "  (2, 17171)\t1\n",
      "  (2, 19718)\t1\n",
      "  (2, 22770)\t1\n",
      "  (2, 23110)\t1\n",
      "  (2, 23428)\t1\n",
      "  (2, 23868)\t1\n",
      "  (2, 28487)\t1\n",
      "  (2, 32219)\t1\n",
      "  (2, 33695)\t1\n",
      "  (2, 36347)\t2\n",
      "  (2, 37674)\t1\n",
      "  :\t:\n",
      "  (625579, 16600)\t1\n",
      "  (625579, 20207)\t1\n",
      "  (625579, 21312)\t1\n",
      "  (625579, 24284)\t1\n",
      "  (625579, 28577)\t1\n",
      "  (625579, 28890)\t1\n",
      "  (625579, 29905)\t1\n",
      "  (625579, 31072)\t1\n",
      "  (625579, 32641)\t1\n",
      "  (625579, 36634)\t1\n",
      "  (625579, 39875)\t1\n",
      "  (625579, 42031)\t1\n",
      "  (625579, 44912)\t1\n",
      "  (625579, 46370)\t1\n",
      "  (625579, 46647)\t1\n",
      "  (625579, 49290)\t3\n",
      "  (625579, 50050)\t1\n",
      "  (625579, 50449)\t1\n",
      "  (625579, 50746)\t1\n",
      "  (625579, 53999)\t1\n",
      "  (625579, 54472)\t1\n",
      "  (625579, 54633)\t1\n",
      "  (625579, 55170)\t1\n",
      "  (625579, 55314)\t1\n",
      "  (625580, 17408)\t1\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TF-IDF Vectors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Level TF-IDF\n",
      " Time taken: 0 hours 1 minutes and 9.53 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Word Level TF-IDF #: Matrix represents tf-idf scores of every term in each review \n",
    "tfidf_word = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_word.fit(df['stemmed'])\n",
    "xtrain_tfidf_word = tfidf_word.transform(train_x)\n",
    "xvalid_tfidf_word = tfidf_word.transform(valid_x)\n",
    "\n",
    "print(\"Word Level TF-IDF\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram, Bigram, Trigram TF-IDF:\n",
      " Time taken: 0 hours 5 minutes and 8.71 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Unigram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram = tfidf_unigram.transform(train_x)\n",
    "xvalid_tfidf_unigram = tfidf_unigram.transform(valid_x)\n",
    "\n",
    "# Bigram Level TF-IDF #: Terms are grouped together by twos \n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_bigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_bigram = tfidf_bigram.transform(train_x)\n",
    "xvalid_tfidf_bigram = tfidf_bigram.transform(valid_x)\n",
    "\n",
    "# Trigram Level TF-IDF #: Terms are grouped together in threes \n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_trigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_trigram = tfidf_trigram.transform(train_x)\n",
    "xvalid_tfidf_trigram = tfidf_trigram.transform(valid_x)\n",
    "\n",
    "print(\"Unigram, Bigram, Trigram TF-IDF:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4411)\t0.4576933239103641\n",
      "  (0, 4388)\t0.2944212060537047\n",
      "  (0, 4310)\t0.28229518398521697\n",
      "  (0, 2962)\t0.21183835045414057\n",
      "  (0, 2474)\t0.32590584463074085\n",
      "  (0, 2133)\t0.2494028458757174\n",
      "  (0, 1003)\t0.42973576119395684\n",
      "  (0, 194)\t0.47558142678730936\n",
      "  (1, 1501)\t0.5471795605489724\n",
      "  (1, 130)\t0.8370152498714902\n",
      "  (2, 4786)\t0.07618026563249647\n",
      "  (2, 4743)\t0.08732790547926773\n",
      "  (2, 4696)\t0.1718860077237478\n",
      "  (2, 4554)\t0.29825644750067865\n",
      "  (2, 4389)\t0.062208280070648436\n",
      "  (2, 4223)\t0.22665233453579417\n",
      "  (2, 4120)\t0.08506546618465322\n",
      "  (2, 3252)\t0.20496306703249728\n",
      "  (2, 3135)\t0.5382438336890729\n",
      "  (2, 2895)\t0.1304426862639404\n",
      "  (2, 2781)\t0.19644653108750007\n",
      "  (2, 2474)\t0.14711673786505555\n",
      "  (2, 2101)\t0.09636996916686696\n",
      "  (2, 2052)\t0.26294155882917025\n",
      "  (2, 2026)\t0.1183182355573233\n",
      "  :\t:\n",
      "  (625579, 4495)\t0.2416933949684209\n",
      "  (625579, 4458)\t0.08474856642238468\n",
      "  (625579, 4389)\t0.19310606761460686\n",
      "  (625579, 4120)\t0.08801967436953376\n",
      "  (625579, 4097)\t0.2601575017524426\n",
      "  (625579, 3936)\t0.18153477125647513\n",
      "  (625579, 3651)\t0.07732965252605381\n",
      "  (625579, 3471)\t0.15354448741367538\n",
      "  (625579, 3151)\t0.16666020498707138\n",
      "  (625579, 2817)\t0.14704896978512505\n",
      "  (625579, 2687)\t0.1839839414674067\n",
      "  (625579, 2587)\t0.2531204540400159\n",
      "  (625579, 2505)\t0.09068293632203414\n",
      "  (625579, 2482)\t0.23615907537078568\n",
      "  (625579, 2133)\t0.11649245916864083\n",
      "  (625579, 1846)\t0.3150365242630891\n",
      "  (625579, 1764)\t0.12212352167307296\n",
      "  (625579, 1443)\t0.20696710564329726\n",
      "  (625579, 937)\t0.22610620172379656\n",
      "  (625579, 868)\t0.20211790679207164\n",
      "  (625579, 399)\t0.12090348631986378\n",
      "  (625579, 168)\t0.27352852149339746\n",
      "  (625579, 138)\t0.16301700975778927\n",
      "  (625579, 86)\t0.17784626407814508\n",
      "  (625580, 1501)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_tfidf_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(xtrain_tfidf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Level TF-IDF:\n",
      " Time taken: 0 hours 8 minutes and 49.03 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Character Level TF-IDF #: Matrix represents tf-idf scores of character level uni, bi & tri-gram of all reviews\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=5000)\n",
    "tfidf_char.fit(df['stemmed'])\n",
    "xtrain_tfidf_char =  tfidf_char.transform(train_x) \n",
    "xvalid_tfidf_char =  tfidf_char.transform(valid_x) \n",
    "\n",
    "print(\"Character Level TF-IDF:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Model # \n",
    "model = xgboost.XGBClassifier(max_depth=7,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.2,\n",
    "                           n_estimators=500,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0,\n",
    "                           max_delta_step=0,\n",
    "                           subsample=1,\n",
    "                           colsample_bytree=1,\n",
    "                           colsample_bylevel=1,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=0,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=1,\n",
    "                           missing=None,\n",
    "                           tree_method='exact',\n",
    "                           nthread=4)\n",
    "\n",
    "# Params for hyperparameter grid search # \n",
    "params = {\n",
    "        'max_depth': [5,7],\n",
    "        'min_child_weight': [1, 5],\n",
    "        'gamma': [0.5, 1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.9293472787696557\n",
      "\n",
      " Time taken: 0 hours 2 minutes and 6.38 seconds.\n",
      "Xgb, WordLevel TF-IDF:  0.9317450498017044\n",
      "\n",
      " Time taken: 0 hours 2 minutes and 50.55 seconds.\n",
      "Xgb, Unigram TF-IDF:  0.9317450498017044\n",
      "\n",
      " Time taken: 0 hours 2 minutes and 48.8 seconds.\n",
      "Xgb, Bigram TF-IDF:  0.8570784598637107\n",
      "\n",
      " Time taken: 0 hours 1 minutes and 26.16 seconds.\n",
      "Xgb, Trigram TF-IDF:  0.7381394255899716\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 39.41 seconds.\n",
      "Xgb, CharLevel Vectors:  0.936516614155481\n",
      "\n",
      " Time taken: 0 hours 20 minutes and 40.38 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_word.tocsc(), train_y, xvalid_tfidf_word.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_unigram.tocsc(), train_y, xvalid_tfidf_unigram.tocsc())\n",
    "print (\"Xgb, Unigram TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_bigram.tocsc(), train_y, xvalid_tfidf_bigram.tocsc())\n",
    "print (\"Xgb, Bigram TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_trigram.tocsc(), train_y, xvalid_tfidf_trigram.tocsc())\n",
    "print (\"Xgb, Trigram TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_char.tocsc(), train_y, xvalid_tfidf_char.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Grid Search with 3 Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  24 out of  24 | elapsed: 78.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0.5, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=0, max_depth=7,\n",
      "              min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=4, nthread=4, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=1, reg_alpha=0,\n",
      "              reg_lambda=0, scale_pos_weight=1, seed=1, silent=True,\n",
      "              subsample=1, tree_method='exact', validate_parameters=False,\n",
      "              verbosity=None)\n",
      "\n",
      " Best score:\n",
      "0.9425094432215813\n",
      "\n",
      " Best parameters:\n",
      "{'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 5}\n",
      "\n",
      " Time taken: 1 hours 26 minutes and 20.57 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# 3 fold validation with hyperparameter grid search #\n",
    "folds = 3\n",
    "param_comb = 3\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle = False, random_state = None)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', n_jobs=3, cv=kf.split(xtrain_count,train_y), \n",
    "                    verbose=3 )\n",
    "\n",
    "start_time = timer(None)\n",
    "\n",
    "grid.fit(xtrain_count,train_y)\n",
    "\n",
    "print('\\n Best estimator:')\n",
    "print(grid.best_estimator_)\n",
    "print('\\n Best score:')\n",
    "print(grid.best_score_)\n",
    "print('\\n Best parameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to door the call no hour access it up wall',\n",
       " 'vegetarian choic option at least tast breakfast etc shop perhap',\n",
       " 'interior theatreland mass charact style and a for with wa',\n",
       " 'chair outlin terranc meter written premium deep a half the',\n",
       " 'a and bar stay attract have better local for t',\n",
       " 'some isn unit beat noisi upgrad my bread white bap',\n",
       " 'tower grang closer fanciest weren either quadrupl slightli hill thought',\n",
       " 'la hop rambla storag plastic catalunya ceram plaza goog spectacular',\n",
       " 'headboard a includ bed with breakfast curtain gbp appoint english',\n",
       " 'gener bit a expens to difficult car the hotel thi',\n",
       " 'bedsid light use would been have a olllld menu frontag',\n",
       " 'a bit the from far to but expens citi center',\n",
       " 'slipper dirtili toothbrush toothpast bathrob no bathroom and a beauti',\n",
       " 'a the and to in hotel wa room with beauti',\n",
       " 'isol less certainli ac post until bedsid becom waterfront supermarket',\n",
       " 'water bottl tap tabl just sad custom whi guest watch',\n",
       " 'the and wa i a had from room it with',\n",
       " 'taxi later budget fair aren cheap fault the here again',\n",
       " 'monoment knife consumpt than box reason group lack break enough',\n",
       " 'lift genuin minor low charger phone guest issu onli so']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LDA Model # \n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vector.get_feature_names()\n",
    "\n",
    "# View Topic Models # \n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "\n",
    "topic_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.952\n",
      "NB, WordLevel TF-IDF:  0.9\n",
      "NB, Uni-Gram Vectors:  0.9\n",
      "NB, Bi-Gram Vectors:  0.916\n",
      "NB, Tri-Gram Vectors:  0.844\n",
      "NB, CharLevel Vectors:  0.84\n"
     ]
    }
   ],
   "source": [
    "# Assumes indepedence among predictors # \n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"NB, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"NB, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"NB, Tri-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier (Logistic Regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.956\n",
      "LR, WordLevel TF-IDF:  0.932\n",
      "LR, Uni-Gram Vectors:  0.932\n",
      "LR, Bi-Gram Vectors:  0.884\n",
      "LR, Tri-Gram Vectors:  0.832\n",
      "LR, CharLevel Vectors:  0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"LR, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"LR, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"LR, Tri-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Level Vectors:  0.82\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 0.09 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "# SVM on Count Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM, Count Level Vectors: \", accuracy)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Word Level Vectors:  0.816\n",
      "SVM, Uni-Gram Vectors:  0.816\n",
      "SVM, Bi-Gram Vectors:  0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Tri-Gram Vectors:  0.816\n",
      "SVM, CharLevel Vectors:  0.816\n"
     ]
    }
   ],
   "source": [
    "# Supervised ML Algo that extracts best possible hyper-plane/ line that segregates the two classes #\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"SVM, Word Level Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"SVM, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"SVM, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"SVM, Tri-Gram Vectors: \", accuracy)\n",
    "       \n",
    "# SVM on Char Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"SVM, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.952\n",
      "RF, Word Level Vectors:  0.952\n",
      "RF, Uni-gram TF-IDF:  0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Bi-gram TF-IDF:  0.944\n",
      "RF, Tri-gram TF-IDF:  0.9\n",
      "RF, CharLevel TF-IDF:  0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"RF, Word Level Vectors: \", accuracy)\n",
    "\n",
    "# RF on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"RF, Uni-gram TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"RF, Bi-gram TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"RF, Tri-gram TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Char Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"RF, CharLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Importance XGB \n",
    "###### Can't plot important features if we only have one train column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code for future reference # \n",
    "# Plot graph showing importance features, max = 50 features # \n",
    "importance = xgboost.XGBClassifier().feature_importances_ \n",
    "importance = pd.Series(importance, index=xtrain_count.columns)\n",
    "importance.nlargest(50).plot(kind='barh')\n",
    "\n",
    "# Select important features #\n",
    "importance.sort_values(axis=0,ascending=False, inplace=True)\n",
    "selected_features = importance.index[0:30].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
