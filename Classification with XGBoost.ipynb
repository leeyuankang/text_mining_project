{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import decomposition, ensemble\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim import corpora\n",
    "\n",
    "import pandas as pd \n",
    "import nltk as nltk\n",
    "import  xgboost, numpy, string\n",
    "import datetime as dt\n",
    "import re as re\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer to check execution timing for each function call # \n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = dt.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((dt.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "# Spelling checker # \n",
    "def spell_correct(array):\n",
    "    spell = SpellChecker()\n",
    "    for i in range(len(array)):\n",
    "        array[i] = spell.correction(array[i])\n",
    "    return array\n",
    "    \n",
    "def stem(array):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in array]\n",
    "\n",
    "def lemmetize(array):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    return [lemmatizer.lemmatize(w) for w in array]\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # Fit the training dataset onto classifier #\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # Predict the labels on validation dataset #\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    return metrics.accuracy_score(predictions, valid_y)\n",
    "\n",
    "def predictions(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # Fit the training dataset onto classifier #\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # Predict the labels on validation dataset #\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    return [classifier, predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer: \n",
      " Time taken: 0 hours 0 minutes and 0.09 seconds.\n",
      "\n",
      "Stemming: \n",
      " Time taken: 0 hours 0 minutes and 0.16 seconds.\n",
      "\n",
      "Lemmetization: \n",
      " Time taken: 0 hours 0 minutes and 0.03 seconds.\n",
      "\n",
      "Train-Test Split: \n",
      " Time taken: 0 hours 0 minutes and 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Data Prep# \n",
    "df = pd.read_csv(\"data/cleaned_hotelreviews_short.csv\")\n",
    "\n",
    "# Drop rows with null comments # \n",
    "df = df.dropna(subset=['reviews'])\n",
    "\n",
    "#####################################################\n",
    "# Remove stop words # \n",
    "stop_list = stopwords.words('english')\n",
    "df['reviews'] = df['reviews'].apply(lambda x: [word for word in x.split() if word not in stop_list])\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# Remove single words # \n",
    "df['reviews'] = df['reviews'].apply(lambda x: x if len(x) > 1 else [])\n",
    "\n",
    "# Drop rows where reviews == [] # \n",
    "df = df[df.reviews.str.len()>0]\n",
    "\n",
    "# Make words case-insensitive # \n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# Remove punctuations if any # \n",
    "df[\"words_only\"] = df['reviews'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Tokenization with NLTK # \n",
    "start_time = timer(None)\n",
    "df['tokenized'] = df['words_only'].apply(nltk.word_tokenize)\n",
    "print(\"\\nTokenizer: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Spelling checker # : Replace incorrect words with correct words \n",
    "#start_time = timer(None)\n",
    "#df['corrected'] = df['tokenized'].apply(spell_correct)\n",
    "#print(\"\\nSpelling Correction: \",end=\"\")\n",
    "#timer(start_time)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "start_time = timer(None)\n",
    "df['stemmed'] = df['tokenized'].apply(stem)\n",
    "print(\"\\nStemming: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Lemmetisation # \n",
    "start_time = timer(None)\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "df['lemmetized'] = df['stemmed'].apply(lemmetize)\n",
    "print(\"\\nLemmetization: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "df['lemmetized'] = df['lemmetized'].apply(\" \".join)\n",
    "\n",
    "# Train - Test Split # \n",
    "start_time = timer(None)\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['lemmetized'], df['class'])\n",
    "print(\"\\nTrain-Test Split: \",end=\"\")\n",
    "timer(start_time)\n",
    "\n",
    "# Label encode target variable to run ML models # \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count vectorisation : Create vectors as features \n",
    "    # Every row represents a review \n",
    "    # Every column represents a term from the corpus \n",
    "    # Every cell represents the frequency count of the particular term in the particular review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectors:\n",
      " Time taken: 0 hours 0 minutes and 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Create count vectoriser object # \n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['lemmetized'])\n",
    "\n",
    "# Transform training and validation data # \n",
    "xtrain_count = count_vector.transform(train_x)\n",
    "xvalid_count = count_vector.transform(valid_x)\n",
    "\n",
    "print(\"Count Vectors:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 165)\t1\n",
      "  (0, 178)\t1\n",
      "  (0, 433)\t1\n",
      "  (0, 438)\t1\n",
      "  (0, 618)\t1\n",
      "  (0, 695)\t1\n",
      "  (0, 892)\t1\n",
      "  (0, 973)\t1\n",
      "  (0, 1244)\t1\n",
      "  (0, 1452)\t1\n",
      "  (0, 1515)\t1\n",
      "  (0, 1621)\t2\n",
      "  (1, 165)\t1\n",
      "  (1, 246)\t1\n",
      "  (1, 284)\t1\n",
      "  (1, 478)\t2\n",
      "  (1, 570)\t1\n",
      "  (1, 1140)\t1\n",
      "  (1, 1436)\t1\n",
      "  (1, 1528)\t1\n",
      "  (1, 1532)\t1\n",
      "  (2, 110)\t1\n",
      "  (2, 111)\t1\n",
      "  (2, 578)\t1\n",
      "  (2, 910)\t1\n",
      "  :\t:\n",
      "  (245, 71)\t1\n",
      "  (245, 74)\t1\n",
      "  (245, 119)\t1\n",
      "  (245, 151)\t1\n",
      "  (245, 188)\t1\n",
      "  (245, 250)\t1\n",
      "  (245, 516)\t1\n",
      "  (245, 591)\t1\n",
      "  (245, 864)\t1\n",
      "  (245, 883)\t1\n",
      "  (245, 920)\t1\n",
      "  (245, 960)\t1\n",
      "  (245, 961)\t1\n",
      "  (245, 989)\t2\n",
      "  (245, 1031)\t1\n",
      "  (245, 1186)\t1\n",
      "  (245, 1286)\t1\n",
      "  (245, 1301)\t1\n",
      "  (245, 1377)\t1\n",
      "  (245, 1602)\t1\n",
      "  (245, 1644)\t1\n",
      "  (245, 1678)\t4\n",
      "  (246, 165)\t1\n",
      "  (246, 570)\t1\n",
      "  (246, 1496)\t1\n"
     ]
    }
   ],
   "source": [
    "print(xvalid_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TF-IDF Vectors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Level TF-IDF\n",
      " Time taken: 0 hours 0 minutes and 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Word Level TF-IDF #: Matrix represents tf-idf scores of every term in each review \n",
    "tfidf_word = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_word.fit(df['lemmetized'])\n",
    "xtrain_tfidf_word = tfidf_word.transform(train_x)\n",
    "xvalid_tfidf_word = tfidf_word.transform(valid_x)\n",
    "\n",
    "print(\"Word Level TF-IDF\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram, Bigram, Trigram TF-IDF:\n",
      " Time taken: 0 hours 0 minutes and 0.08 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Unigram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_unigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_unigram = tfidf_unigram.transform(train_x)\n",
    "xvalid_tfidf_unigram = tfidf_unigram.transform(valid_x)\n",
    "\n",
    "# Bigram Level TF-IDF #: Terms are grouped together by twos \n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_bigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_bigram = tfidf_bigram.transform(train_x)\n",
    "xvalid_tfidf_bigram = tfidf_bigram.transform(valid_x)\n",
    "\n",
    "# Trigram Level TF-IDF #: Terms are grouped together in threes \n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_trigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_trigram = tfidf_trigram.transform(train_x)\n",
    "xvalid_tfidf_trigram = tfidf_trigram.transform(valid_x)\n",
    "\n",
    "print(\"Unigram, Bigram, Trigram TF-IDF:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xtrain_tfidf_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Level TF-IDF:\n",
      " Time taken: 0 hours 0 minutes and 0.13 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# Character Level TF-IDF #: Matrix represents tf-idf scores of character level uni, bi & tri-gram of all reviews\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=5000)\n",
    "tfidf_char.fit(df['lemmetized'])\n",
    "xtrain_tfidf_char =  tfidf_char.transform(train_x) \n",
    "xvalid_tfidf_char =  tfidf_char.transform(valid_x) \n",
    "\n",
    "print(\"Character Level TF-IDF:\", end=\"\")\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Model # \n",
    "model = xgboost.XGBClassifier(max_depth=7,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.2,\n",
    "                           n_estimators=500,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0,\n",
    "                           max_delta_step=0,\n",
    "                           subsample=1,\n",
    "                           colsample_bytree=1,\n",
    "                           colsample_bylevel=1,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=0,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=1,\n",
    "                           missing=None,\n",
    "                           tree_method='exact',\n",
    "                           nthread=4)\n",
    "\n",
    "# Params for hyperparameter grid search # \n",
    "params = {\n",
    "        'max_depth': [5,7],\n",
    "        'min_child_weight': [1, 5],\n",
    "        'gamma': [0.5, 1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.9757085020242915\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 0.74 seconds.\n",
      "Xgb, WordLevel TF-IDF:  0.9676113360323887\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 0.76 seconds.\n",
      "Xgb, Unigram TF-IDF:  0.9676113360323887\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 0.75 seconds.\n",
      "Xgb, Bigram TF-IDF:  0.8987854251012146\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 1.82 seconds.\n",
      "Xgb, Trigram TF-IDF:  0.8137651821862348\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 0.62 seconds.\n",
      "Xgb, CharLevel Vectors:  0.97165991902834\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 1.62 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_word.tocsc(), train_y, xvalid_tfidf_word.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_unigram.tocsc(), train_y, xvalid_tfidf_unigram.tocsc())\n",
    "print (\"Xgb, Unigram TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_bigram.tocsc(), train_y, xvalid_tfidf_bigram.tocsc())\n",
    "print (\"Xgb, Bigram TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_trigram.tocsc(), train_y, xvalid_tfidf_trigram.tocsc())\n",
    "print (\"Xgb, Trigram TF-IDF: \", accuracy)\n",
    "timer(start_time)\n",
    "\n",
    "start_time = timer(None)\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_char.tocsc(), train_y, xvalid_tfidf_char.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       199\n",
      "           1       1.00      0.88      0.93        48\n",
      "\n",
      "    accuracy                           0.98       247\n",
      "   macro avg       0.99      0.94      0.96       247\n",
      "weighted avg       0.98      0.98      0.98       247\n",
      "\n",
      "\n",
      "Confussion matrix:\n",
      " [[199   0]\n",
      " [  6  42]]\n"
     ]
    }
   ],
   "source": [
    "model = xgboost.XGBClassifier().fit(xtrain_count.tocsc(), train_y)\n",
    "\n",
    "predictions = model.predict(xvalid_count.tocsc())\n",
    "\n",
    "# Confusion matrix # \n",
    "confusion = confusion_matrix(valid_y, predictions)\n",
    "class_report = classification_report(valid_y, predictions)\n",
    "\n",
    "print('\\nClasification report:\\n', class_report)\n",
    "print('\\nConfussion matrix:\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter review :this place is a good place to stay\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter review :\")\n",
    "user_input = user_input.split()\n",
    "\n",
    "# Remove stop words # \n",
    "user_input = [word for word in user_input if word not in stop_list]\n",
    "\n",
    "# Make words case-insensitive # \n",
    "user_input = [word.lower() for word in user_input]\n",
    "\n",
    "# Remove punctuations if any # \n",
    "user_input = [re.sub('[^\\w\\s]','', word) for word in user_input]\n",
    "\n",
    "# Spelling checker # : Replace incorrect words with correct words \n",
    "user_input = spell_correct(user_input)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "user_input = stem(user_input)\n",
    "\n",
    "# Lemmetisation # \n",
    "user_input = lemmetize(user_input)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "user_input = \" \".join(user_input)\n",
    "user_input = pd.Series(user_input)\n",
    "\n",
    "user_valid = valid_x.append(user_input, ignore_index=False).rename(\"lemmetized\")\n",
    "user_valid = user_valid.iloc[-1:]\n",
    "\n",
    "#print(user_valid)\n",
    "\n",
    "# Count Vector # \n",
    "user_count = count_vector.transform(user_input)\n",
    "\n",
    "user_predictions = model.predict(user_count.tocsc())\n",
    "\n",
    "if user_predictions == 0:\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Grid Search with 3 Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  24 out of  24 | elapsed:   22.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0.5, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=0, max_depth=7,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=4, nthread=4, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=1, reg_alpha=0,\n",
      "              reg_lambda=0, scale_pos_weight=1, seed=1, silent=True,\n",
      "              subsample=1, tree_method='exact', validate_parameters=False,\n",
      "              verbosity=None)\n",
      "\n",
      " Best score:\n",
      "0.9546061415220294\n",
      "\n",
      " Best parameters:\n",
      "{'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 1}\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 23.92 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "\n",
    "# 3 fold validation with hyperparameter grid search #\n",
    "folds = 3\n",
    "param_comb = 3\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle = False, random_state = None)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', n_jobs=3, cv=kf.split(xtrain_count,train_y), \n",
    "                    verbose=3 )\n",
    "\n",
    "start_time = timer(None)\n",
    "\n",
    "grid.fit(xtrain_count,train_y)\n",
    "\n",
    "print('\\n Best estimator:')\n",
    "print(grid.best_estimator_)\n",
    "print('\\n Best score:')\n",
    "print(grid.best_score_)\n",
    "print('\\n Best parameters:')\n",
    "print(grid.best_params_)\n",
    "\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['might suffici bit date look centr although clean nice bigger',\n",
       " 'accessori toilet rate also bit breakfast room far includ seem',\n",
       " 'bit far expens citi center centr away room station tram',\n",
       " 'room check staff water tv recept two like bottl london',\n",
       " 'air swim rooftop sunbath barra rel condition doubletre compet option',\n",
       " 'screen bike bit user away center rent tv perfect take',\n",
       " 'around edg knock tram environ renov produc forc complimentari will',\n",
       " 'ad touch hotel matter bill bar even approach sm station',\n",
       " 'onsit nice bar would back bathroom accessori gull fahrenheit summer',\n",
       " 'bar would nice area plenti anyway baggi premis soap slightli',\n",
       " 'big tea facil thank thought book even barcelona hotel stay',\n",
       " 'bath would nice bathroom room shower addit expect made relax',\n",
       " 'hotel beauti room staff great help breakfast locat stay bed',\n",
       " 'factori often impress kind hotel small choic give one person',\n",
       " 'bit far cost attract main high correspond tool may complex',\n",
       " 'better would bigger room breakfast small nice one choic good',\n",
       " 'utensil combin flat typic realli point minor bag tea smile',\n",
       " 'hotel beauti park close room find view balconi canal street',\n",
       " 'bit expens town bu tram enough far close probabl day',\n",
       " 'pari hotel aspect look competit regularli similar our powel russel']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LDA Model # \n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vector.get_feature_names()\n",
    "\n",
    "# View Topic Models # \n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "\n",
    "topic_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.94\n",
      "NB, WordLevel TF-IDF:  0.904\n",
      "NB, Uni-Gram Vectors:  0.904\n",
      "NB, Bi-Gram Vectors:  0.82\n",
      "NB, Tri-Gram Vectors:  0.808\n",
      "NB, CharLevel Vectors:  0.852\n"
     ]
    }
   ],
   "source": [
    "# Assumes indepedence among predictors # \n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"NB, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"NB, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"NB, Tri-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier (Logistic Regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.944\n",
      "LR, WordLevel TF-IDF:  0.916\n",
      "LR, Uni-Gram Vectors:  0.916\n",
      "LR, Bi-Gram Vectors:  0.824\n",
      "LR, Tri-Gram Vectors:  0.808\n",
      "LR, CharLevel Vectors:  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"LR, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"LR, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"LR, Tri-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Level Vectors:  0.808\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "start_time = timer(None)\n",
    "# SVM on Count Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM, Count Level Vectors: \", accuracy)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Word Level Vectors:  0.804\n",
      "SVM, Uni-Gram Vectors:  0.804\n",
      "SVM, Bi-Gram Vectors:  0.804\n",
      "SVM, Tri-Gram Vectors:  0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors:  0.804\n"
     ]
    }
   ],
   "source": [
    "# Supervised ML Algo that extracts best possible hyper-plane/ line that segregates the two classes #\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"SVM, Word Level Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Unigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_unigram, train_y, xvalid_tfidf_unigram)\n",
    "print (\"SVM, Uni-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Bigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_bigram, train_y, xvalid_tfidf_bigram)\n",
    "print (\"SVM, Bi-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Trigram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_trigram, train_y, xvalid_tfidf_trigram)\n",
    "print (\"SVM, Tri-Gram Vectors: \", accuracy)\n",
    "       \n",
    "# SVM on Char Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_char, train_y, xvalid_tfidf_char)\n",
    "print (\"SVM, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.9473684210526315\n",
      "RF, WordLevel TF-IDF:  0.9635627530364372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_word, train_y, xvalid_tfidf_word)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments and signature arguments do not match. got: 19, expected: 21 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-5724524b41c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdense_xtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxtrain_tfidf_bigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense_xtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    525\u001b[0m           \u001b[1;34m\"Arguments and signature arguments do not match. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m           \u001b[1;34m\"got: %s, expected: %s \"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m           (len(args), len(list(self.signature.input_arg))))\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[0mfunction_call_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments and signature arguments do not match. got: 19, expected: 21 "
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_bigram.shape[1])\n",
    "\n",
    "dense_xtrain = xtrain_tfidf_bigram.toarray()\n",
    "\n",
    "classifier.fit(dense_xtrain, train_y)\n",
    "\n",
    "#accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Importance XGB \n",
    "###### Can't plot important features if we only have one train column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code for future reference # \n",
    "# Plot graph showing importance features, max = 50 features # \n",
    "importance = xgboost.XGBClassifier().feature_importances_ \n",
    "importance = pd.Series(importance, index=xtrain_count.columns)\n",
    "importance.nlargest(50).plot(kind='barh')\n",
    "\n",
    "# Select important features #\n",
    "importance.sort_values(axis=0,ascending=False, inplace=True)\n",
    "selected_features = importance.index[0:30].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
