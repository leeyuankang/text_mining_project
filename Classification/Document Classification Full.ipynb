{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries # \n",
    "import pandas as pd \n",
    "import nltk as nltk\n",
    "import  xgboost, numpy, string\n",
    "import datetime as dt\n",
    "import re as re\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions # \n",
    "\n",
    "# Timer to check execution timing for each function call # \n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = dt.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((dt.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "def stem(array):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in array]\n",
    "\n",
    "def lemmetize(array):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    return [lemmatizer.lemmatize(w) for w in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Data Prep # \n",
    "df = pd.read_csv(\"../data/cleaned_hotelreviews.csv\")\n",
    "\n",
    "# Drop rows with null comments # \n",
    "df = df.dropna(subset=['reviews'])\n",
    "\n",
    "# Make words case-insensitive # \n",
    "df = df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Accuracy:  0.9275155555022719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  0.9391147450576063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:  0.9248480416252053\n",
      "Naive Bayes Accuracy:  0.9193391758880723\n"
     ]
    }
   ],
   "source": [
    "# Run baseline models based on Count Vector # \n",
    "\n",
    "# Train - Test Split # \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['reviews'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable # \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# Create Count Vector #  \n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['reviews'])\n",
    "\n",
    "# Transform training and validation data # \n",
    "xtrain_count = count_vector.transform(train_x)\n",
    "xvalid_count = count_vector.transform(valid_x)\n",
    "\n",
    "# XG Boost on Unprocessed Data # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_count.tocsc(), train_y)\n",
    "predictions = model.predict(xvalid_count.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"XGBoost Classifier Accuracy: \", accuracy)\n",
    "\n",
    "# Logistic Regression on Unprocessed Data # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Logistic Regression Accuracy: \", accuracy)\n",
    "\n",
    "# SVM on Unprocessed Data # \n",
    "#model = svm.SVC().fit(xtrain_count, train_y)\n",
    "#predictions = model.predict(xvalid_count)\n",
    "#accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "#print(\"SVM Accuracy: \", accuracy)\n",
    "\n",
    "# Random Forest on Unprocessed Data # \n",
    "model  = ensemble.RandomForestClassifier().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Random Forest Accuracy: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Unprocessed Data # \n",
    "model = naive_bayes.MultinomialNB().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Naive Bayes Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Data Prep on two best models # \n",
    "\n",
    "# Remove punctuations if any # \n",
    "df[\"words_only\"] = df['reviews'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Remove stop words # \n",
    "stop_list = stopwords.words('english')\n",
    "df['reviews'] = df['reviews'].apply(lambda x: [word for word in x.split() if word not in stop_list])\n",
    "\n",
    "# Remove single words # \n",
    "df['reviews'] = df['reviews'].apply(lambda x: x if len(x) > 1 else [])\n",
    "\n",
    "# Drop rows where reviews == [] # \n",
    "df = df[df.reviews.str.len()>0]\n",
    "\n",
    "# Tokenization with NLTK # \n",
    "df['tokenized'] = df['words_only'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "df['stemmed'] = df['tokenized'].apply(stem)\n",
    "\n",
    "# Turn arrays for each row in df['stemmed'] into a string #: Needed to run SkLearn Lib\n",
    "df['stemmed'] = df['stemmed'].apply(\" \".join)\n",
    "\n",
    "# Lemmetisation # \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "df['lemmetized'] = df['tokenized'].apply(lemmetize)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "df['lemmetized'] = df['lemmetized'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test Split for Stemmed Words # \n",
    "train_stemx, valid_stemx, train_stemy, valid_stemy = model_selection.train_test_split(df['stemmed'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable [STEMMED] # \n",
    "train_stemy = encoder.fit_transform(train_stemy)\n",
    "valid_stemy = encoder.fit_transform(valid_stemy)\n",
    "\n",
    "# Train - Test Split for Lemmetized Words # \n",
    "train_lemx, valid_lemx, train_lemy, valid_lemy = model_selection.train_test_split(df['lemmetized'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable [LEMMETIZED] # \n",
    "train_lemy = encoder.fit_transform(train_lemy)\n",
    "valid_lemy = encoder.fit_transform(valid_lemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Stem:  0.9453969978772869\n",
      "XGBoost Stem:  0.9357437076720914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Unigram Stem:  0.9475765692914182\n",
      "XGBoost Unigram Stem:  0.9364702314768018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Bigram Stem:  0.9264631557667037\n",
      "XGBoost Bigram Stem:  0.8768257859092288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Trigram Stem:  0.8366142727180835\n",
      "XGBoost Trigram Stem:  0.8768257859092288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Quadgram Stem:  0.7281411098756697\n",
      "XGBoost Quadgram Stem:  0.6443823915900131\n"
     ]
    }
   ],
   "source": [
    "# Best Models [STEMMED] # \n",
    "\n",
    "# Create Count Vector [STEMMED] #  \n",
    "count_vector.fit(df['stemmed'])\n",
    "xtrain_count_stem = count_vector.transform(train_stemx)\n",
    "xvalid_count_stem = count_vector.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_count_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_count_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_count_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_count_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Stem: \", accuracy)\n",
    "\n",
    "# Ngram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_quadgram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(4,4), max_features=5000)\n",
    "\n",
    "# Unigram [STEMMED] # \n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram_stem = tfidf_unigram.transform(train_stemx)\n",
    "xvalid_tfidf_unigram_stem = tfidf_unigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_unigram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Unigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_unigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Unigram Stem: \", accuracy)\n",
    "\n",
    "# Bigram [STEMMED] # \n",
    "tfidf_bigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_bigram_stem = tfidf_bigram.transform(train_stemx)\n",
    "xvalid_tfidf_bigram_stem = tfidf_bigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_bigram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Bigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_bigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Bigram Stem: \", accuracy)\n",
    "\n",
    "# Trigram [STEMMED] # \n",
    "tfidf_trigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_trigram_stem = tfidf_trigram.transform(train_stemx)\n",
    "xvalid_tfidf_trigram_stem = tfidf_trigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_trigram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Trigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_bigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Trigram Stem: \", accuracy)\n",
    "\n",
    "# Quadgram [STEMMED] # \n",
    "tfidf_quadgram.fit(df['stemmed'])\n",
    "xtrain_tfidf_quadgram_stem = tfidf_quadgram.transform(train_stemx)\n",
    "xvalid_tfidf_quadgram_stem = tfidf_quadgram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_quadgram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Quadgram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_quadgram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Quadgram Stem: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Lem:  0.9464899423835035\n",
      "XGBoost Lem:  0.9342211664813505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Unigram Lem:  0.9490991104821591\n",
      "XGBoost Unigram Lem:  0.935718437278884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Bigram Lem:  0.9252122713029415\n",
      "Xgboost Bigram Lem:  0.8740081370666127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Trigram Lem:  0.8354202466390377\n",
      "XGBoost Trigram Lem:  0.7396328211866977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Quadgram Lem:  0.7264922167188922\n",
      "XGBoost Quadgram Lem:  0.6434031638532296\n"
     ]
    }
   ],
   "source": [
    "# Best Models [LEMMETIZED] # \n",
    "\n",
    "# Create Count Vector [LEMMETIZED] #  \n",
    "count_vector.fit(df['lemmetized'])\n",
    "xtrain_count_lem = count_vector.transform(train_lemx)\n",
    "xvalid_count_lem = count_vector.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_count_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_count_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_count_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_count_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Lem: \", accuracy)\n",
    "\n",
    "# Ngram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_quadgram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(4,4), max_features=5000)\n",
    "\n",
    "# Unigram [LEMMETIZED] # \n",
    "tfidf_unigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_unigram_lem = tfidf_unigram.transform(train_lemx)\n",
    "xvalid_tfidf_unigram_lem = tfidf_unigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_unigram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Unigram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_unigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Unigram Lem: \", accuracy)\n",
    "\n",
    "# Bigram [LEMMETIZED] # \n",
    "tfidf_bigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_bigram_lem = tfidf_bigram.transform(train_lemx)\n",
    "xvalid_tfidf_bigram_lem = tfidf_bigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_bigram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Bigram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_bigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Xgboost Bigram Lem: \", accuracy)\n",
    "\n",
    "# Trigram [LEMMETIZED] # \n",
    "tfidf_trigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_trigram_lem = tfidf_trigram.transform(train_lemx)\n",
    "xvalid_tfidf_trigram_lem = tfidf_trigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_trigram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Trigram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_trigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Trigram Lem: \", accuracy)\n",
    "\n",
    "# Quadgram [LEMMETIZED] # \n",
    "tfidf_quadgram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_quadgram_lem = tfidf_quadgram.transform(train_lemx)\n",
    "xvalid_tfidf_quadgram_lem = tfidf_quadgram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_quadgram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Quadgram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_quadgram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Quadgram Lem: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Unigram Stem:  0.9475765692914182\n",
      "XGBoost Unigram Stem:  0.9364702314768018\n"
     ]
    }
   ],
   "source": [
    "# Unigram [STEMMED] # \n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram_stem = tfidf_unigram.transform(train_stemx)\n",
    "xvalid_tfidf_unigram_stem = tfidf_unigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model_1 = linear_model.LogisticRegression().fit(xtrain_tfidf_unigram_stem, train_stemy)\n",
    "predictions = model_1.predict(xvalid_tfidf_unigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Unigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model_2 = xgboost.XGBClassifier().fit(xtrain_tfidf_unigram_stem.tocsc(), train_stemy)\n",
    "predictions = model_2.predict(xvalid_tfidf_unigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Unigram Stem: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model for Future Use, Pickle # \n",
    "import pickle \n",
    "\n",
    "# Save the model to disk # \n",
    "filename1 = 'classification.pkl'\n",
    "with open(filename1, 'wb') as file:  \n",
    "    pickle.dump([model_1,model_2,tfidf_unigram], file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'picke' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-1d53dca858cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlogistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mxgboost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpicke\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0muser_uni\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'picke' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading Model # \n",
    "with open(filename1, 'rb') as file:  \n",
    "    logistic = pickle.load(file)[0]\n",
    "    xgboost = picke.load(file)[1]\n",
    "    user_uni=pickel.load(file)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter review :hello this is great\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter review :\")\n",
    "user_input = user_input.split()\n",
    "\n",
    "# Remove stop words # \n",
    "user_input = [word for word in user_input if word not in stop_list]\n",
    "\n",
    "# Make words case-insensitive # \n",
    "user_input = [word.lower() for word in user_input]\n",
    "\n",
    "# Remove punctuations if any # \n",
    "user_input = [re.sub('[^\\w\\s]','', word) for word in user_input]\n",
    "\n",
    "# Stemming with NLTK # \n",
    "user_input = stem(user_input)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "user_input = \" \".join(user_input)\n",
    "user_input = pd.Series(user_input)\n",
    "\n",
    "user_unigram = user_uni.transform(user_input)\n",
    "\n",
    "#print(user_input)\n",
    "#print(valid_stemx)\n",
    "\n",
    "prediction = logistic.predict(user_unigram)\n",
    "\n",
    "if prediction[0] == 1: \n",
    "    print(\"Positive\")\n",
    "else: \n",
    "    print(\"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
